{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet():    \n",
    "    postingList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "             ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "             ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "             ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "             ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "             ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0, 1, 0, 1, 0, 1]\n",
    "    return postingList,classVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVocabList(dataset):#将训练集中的词语变为无重复词语的词典\n",
    "    vocabSet=set([])\n",
    "    for line in dataset:\n",
    "        vocabSet=vocabSet|set(line)\n",
    "    return list(vocabSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setOfWord2vec(vocabList,inputSet):#生成每行文本的词向量（词袋模型）\n",
    "    returnVec=[0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)]+=1\n",
    "        else:\n",
    "            print(\"the word%s is not found!\"%word)\n",
    "    return returnVec\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNB(trainMatrix,trainCategory):\n",
    "    numTrainDocs=len(trainMatrix)\n",
    "    numWord=len(trainMatrix[0])\n",
    "    pAbusive=sum(trainCategory)/len(trainCategory)#算出1类在所有类别中的概率，即p(c1)\n",
    "    p0Num=np.ones(numWord)#给0类中所有词向量赋初值1，以防止有些词语不存在导致概率为0\n",
    "    p1Num=np.ones(numWord)#给1类中。。。\n",
    "    p0Demon=2#给类别赋初值，以防止训练集中压根没有该类\n",
    "    p1Demon=2#同上\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i]==0:\n",
    "            p0Num+=trainMatrix[i]#p0Num是0类中所有向量的总和+初值向量 \n",
    "            p0Demon+=sum(trainMatrix[i])#p1Num是0类中所有词语的总和+初值\n",
    "        else:\n",
    "            p1Num+=trainMatrix[i]#p1Num是1类中所有向量的总和+初值向量 \n",
    "            p1Demon+=sum(trainMatrix[i])#p1Num是1类中所有词语的总和+初值\n",
    "    p0vec=np.log(p0Num/p0Demon)#算出0类中各词语出现的概率p(wi|c0)\n",
    "    p1vec=np.log(p1Num/p1Demon)#算出1类中各词语出现的概率p(wi|c1)\n",
    "    #取log是因为p0Num/p0Demon中各项数值可能非常小，相乘后可能会溢出，而且取对数可以将后面的乘法操作变为加法操作，在这里取了log，在classifyNB就不用了\n",
    "    return p0vec,p1vec,pAbusive\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify,p0vec,p1vec,pClass1):#vec2Classify是待预测的文本向量\n",
    "    p1=np.sum(vec2Classify*p1vec)+np.log(pClass1)\n",
    "    p0=np.sum(vec2Classify*p0vec)+np.log(1-pClass1)\n",
    "    if p1>p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testingNB():\n",
    "    vec_dic=[]\n",
    "    trainList,trainTag=loadDataSet()\n",
    "#     print(trainList)\n",
    "#     print(trainTag)\n",
    "    dictionary=createVocabList(trainList)\n",
    "    print(dictionary)\n",
    "    for line in trainList:#对于训练集中每行文本进行向量化\n",
    "        vec_dic.append(setOfWord2vec(dictionary,line))#得到训练集文本向量的列表\n",
    "    print(vec_dic)\n",
    "    p0vec,p1vec,pAbusive=trainNB(vec_dic,trainTag)\n",
    "    text1=['my','take']\n",
    "    text2=['him','stupid','stop']\n",
    "    vec1=setOfWords2vec(dictionary,text1)\n",
    "    vec2=setOfWords2vec(dictionary,text2)\n",
    "    result_of_text1=classifyNB(vec1,p0vec,p1vec,pAbusive)\n",
    "    result_of_text2=classifyNB(vec2,p0vec,p1vec,pAbusive)\n",
    "    print(\"text1的分类结果为%d\"%result_of_text1)\n",
    "    print(\"text2的分类结果为%d\"%result_of_text2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['problems', 'ate', 'garbage', 'my', 'to', 'I', 'posting', 'quit', 'mr', 'buying', 'dalmation', 'stop', 'stupid', 'licks', 'has', 'take', 'maybe', 'love', 'worthless', 'dog', 'flea', 'help', 'is', 'cute', 'how', 'not', 'food', 'so', 'steak', 'park', 'please', 'him']\n",
      "[[1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1], [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1], [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]\n",
      "text1的分类结果为0\n",
      "text2的分类结果为1\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    testingNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
